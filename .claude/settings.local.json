{
  "permissions": {
    "allow": [
      "SlashCommand(/sp.adr:*)",
      "Bash(pwsh -File .specify/scripts/powershell/check-prerequisites.ps1 -Json)",
      "Bash(npx create-docusaurus@latest frontend-book classic)",
      "Bash(npx create-docusaurus@latest frontend-book classic --typescript false --force)",
      "Bash(npx @docusaurus/init@latest frontend-book classic)",
      "Bash(npm run build)",
      "Bash(npm start)",
      "Bash(git fetch --all --prune)",
      "Bash(.specify/scripts/powershell/create-new-feature.ps1 -Json \"Module 2 Title: The Digital Twin (Gazebo & Unity)\n\nFocus:\n- Physics-based simulation and environment modeling\n- Digital twins for humanoid robots\n\nModule Topics:\n- Physics simulation (gravity, collisions) in Gazebo\n- High-fidelity visualization and interaction in Unity\n- Sensor simulation: LiDAR, depth cameras, IMUs\n\nDocusaurus Structure:\n- Exactly 3 chapters\n\nTarget Audience:\n- Students with ROS 2 fundamentals\n- New to physics simulation\" --number 2 --short-name \"digital-twin-sim\")",
      "Bash(pwsh -File .specify/scripts/powershell/check-prerequisites.ps1 -Json -RequireTasks -IncludeTasks)",
      "Bash(find docs/modules/003-ai-robot-brain/ -name \"*.md\" -exec grep -l \"\\.\\/\\|\\.\\/\" {} ;)",
      "Bash(pwsh -File .specify/scripts/powershell/create-new-feature.ps1 -Json \"Module 4 Title: Vision-Language-Action (VLA)\n\nFocus:\n- Convergence of LLMs and robotics\n- Translating natural language into physical robot actions\n\nModule Topics:\n- Voice-to-Action using speech recognition (e.g., Whisper-style systems)\n- Language-based cognitive planning for robotics\n- Vision-guided manipulation and action execution\n\nDocusaurus Structure:\n- Exactly 3 chapters\n\nChapter Scope:\n- Chapter 1: Language and voice interfaces for humanoid robots\n- Chapter 2: LLM-driven planning from instructions to ROS 2 actions\n- Chapter 3: Integrated VLA pipeline and autonomous humanoid capstone concept\n\nTarget Audience:\n- Students with ROS 2, simulation, and AI fundamentals\n- Transitioning to embodied intelligence systems\n\nSuccess Criteria:\n- Reader understands VLA as an end-to-end robotics paradigm\n- Reader can explain language-to-action pipelines\n- Module prepares reader for the autonomous humanoid capstone\" -Number 4 -ShortName \"vision-language-action\")",
      "Bash(.specify/scripts/bash/create-new-feature.sh \"Module 4 Title: Vision-Language-Action (VLA)\n\nFocus:\n- Convergence of LLMs and robotics\n- Translating natural language into physical robot actions\n\nModule Topics:\n- Voice-to-Action using speech recognition (e.g., Whisper-style systems)\n- Language-based cognitive planning for robotics\n- Vision-guided manipulation and action execution\n\nDocusaurus Structure:\n- Exactly 3 chapters\n\nChapter Scope:\n- Chapter 1: Language and voice interfaces for humanoid robots\n- Chapter 2: LLM-driven planning from instructions to ROS 2 actions\n- Chapter 3: Integrated VLA pipeline and autonomous humanoid capstone concept\n\nTarget Audience:\n- Students with ROS 2, simulation, and AI fundamentals\n- Transitioning to embodied intelligence systems\n\nSuccess Criteria:\n- Reader understands VLA as an end-to-end robotics paradigm\n- Reader can explain language-to-action pipelines\n- Module prepares reader for the autonomous humanoid capstone\" --number 4 --short-name \"vision-language-action\")",
      "Bash(pwsh -File .specify/scripts/powershell/setup-plan.ps1 -Json)"
    ],
    "deny": [],
    "ask": []
  }
}
